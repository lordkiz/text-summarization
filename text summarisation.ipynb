{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/kizitoegeonu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/kizitoegeonu/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import heapq\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do all text formating in a function and return the formated text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_preprocessing(text):\n",
    "    #do some preprocessing. I have none to do for now\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_sentences(text):\n",
    "    return nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenize words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_words(formatted_text):\n",
    "    return nltk.word_tokenize(formatted_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word frequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_frequencies(formatted_text):\n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    word_freq = dict()\n",
    "    \n",
    "    for word in tokenize_words(formatted_text):\n",
    "        if word not in stop_words:\n",
    "            if word not in word_freq.keys():\n",
    "                word_freq[word] = 1\n",
    "            else:\n",
    "                word_freq[word] += 1\n",
    "                \n",
    "    return word_freq\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Weighted Frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_frequencies(formatted_text):\n",
    "    word_freq = word_frequencies(formatted_text)\n",
    "    maximum_frequency = max(word_freq.values())\n",
    "    \n",
    "    weighted_freq = dict()\n",
    "    \n",
    "    for word in word_freq.keys():\n",
    "        weighted_freq[word] = (word_freq[word]/maximum_frequency)\n",
    "        \n",
    "    return weighted_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the sentence scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate the scores for each sentence by adding weighted frequencies of the words that occur in that particular sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_scores(formatted_text):\n",
    "    weighted_freq = weighted_frequencies(formatted_text)\n",
    "    tokenized_sentences = tokenize_sentences(formatted_text)\n",
    "    \n",
    "    sentence_scores_table = dict()\n",
    "    \n",
    "    for sentence in tokenized_sentences:\n",
    "        if sentence not in sentence_scores_table.keys():\n",
    "            sentence_scores_table[sentence] = 0\n",
    "        for word in nltk.word_tokenize(sentence):\n",
    "            if word in weighted_freq.keys():\n",
    "                if len(sentence.split(' ')) < 30:\n",
    "                    sentence_scores_table[sentence] += weighted_freq[word]\n",
    "                \n",
    "    return sentence_scores_table\n",
    "                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summarize!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    formatted_text = text_preprocessing(text)\n",
    "    sentence_scores_dict = sentence_scores(formatted_text)\n",
    "    \n",
    "    summary_sentences = heapq.nlargest(5, sentence_scores_dict, key=sentence_scores_dict.get)\n",
    "    print(\"summary sentences\", summary_sentences)\n",
    "    \n",
    "    summary = \" \".join(summary_sentences)\n",
    "    \n",
    "    print(\"summary\", summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\" As I write this article, 1,907,223,370 websites are active on the internet and 2,722,460 emails are being sent per second. This is an unbelievably huge amount of data. It is impossible for a user to get insights from such huge volumes of data. Furthermore, a large portion of this data is either redundant or doesn't contain much useful information. The most efficient way to get access to the most important parts of the data, without having to sift through redundant and insignificant data, is to summarize the data in a way that it contains non-redundant and useful information only. The data can be in any form such as audio, video, images, and text. In this article, we will see how we can use automatic text summarization techniques to summarize text data.\n",
    "\n",
    "Text summarization is a subdomain of Natural Language Processing (NLP) that deals with extracting summaries from huge chunks of texts. There are two main types of techniques used for text summarization: NLP-based techniques and deep learning-based techniques. In this article, we will see a simple NLP-based technique for text summarization. We will not use any machine learning library in this article. Rather we will simply use Python's NLTK library for summarizing Wikipedia articles.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "summary sentences ['In this article, we will see how we can use automatic text summarization techniques to summarize text data.', 'The data can be in any form such as audio, video, images, and text.', 'There are two main types of techniques used for text summarization: NLP-based techniques and deep learning-based techniques.', 'In this article, we will see a simple NLP-based technique for text summarization.', \"Furthermore, a large portion of this data is either redundant or doesn't contain much useful information.\"]\n",
      "summary In this article, we will see how we can use automatic text summarization techniques to summarize text data. The data can be in any form such as audio, video, images, and text. There are two main types of techniques used for text summarization: NLP-based techniques and deep learning-based techniques. In this article, we will see a simple NLP-based technique for text summarization. Furthermore, a large portion of this data is either redundant or doesn't contain much useful information.\n"
     ]
    }
   ],
   "source": [
    "summarize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
